---
title: "6.L3"
author: "Sara Knox"
date: "04/20/2020"
output: html_document
---

```{r setup, echo=FALSE, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())

library(plotly)
library(ggplot2)
library(ggpmisc) 

library(readxl)
library(tidyverse)
library(caret)
library(lubridate)
library(REddyProc)
library(tidyverse)
library(dplyr)
library(Rcpp)

siteID <- 'RBM'
Rdir <- "/Users/ziyi/Library/CloudStorage/GoogleDrive-ziyi.tzuyilu@gmail.com/.shortcut-targets-by-id/1txCh9lZ7VGCujXGvBaJCMVnuxT-65q4K/Micromet\ Lab/Projects/2022-RBM Richmond brackish marsh/Flux-tower"
setwd("/Users/ziyi/Library/CloudStorage/GoogleDrive-ziyi.tzuyilu@gmail.com/.shortcut-targets-by-id/1txCh9lZ7VGCujXGvBaJCMVnuxT-65q4K/Micromet Lab/Projects/2022-RBM Richmond brackish marsh/FluxProcessingRBM")

knitr::opts_knit$set(root.dir = Rdir)

```

```{r Load data, echo=FALSE, include=FALSE}
EddyData.F <- fLoadTXTIntoDataframe(paste(Rdir,"/Flux_data/REddyProc_input/for_gap_filling_partitioning",siteID,".txt",sep=''))
```

```{r Gap-fill and partition in REddyProc, echo=FALSE, include=FALSE}
# Converting DOY to integers
#EddyData.F$DoY <- floor(EddyData.F$DoY)


# Add time stamp in POSIX time format -------------------------------------
EddyDataWithPosix.F <- fConvertTimeToPosix(EddyData.F, 'YDH',Year.s = 'Year',Day.s = 'DoY',Hour.s = 'Hour')
##tmp <- fConvertTimeToPosix(EddyData.F, 'YDH',Year = 'Year',Day ='DoY',Hour = 'Hour')
##iStart = which(tmp$Hour == 0.5)[1] # first record on a new day
##EddyDataWithPosix.F <- tmp[iStart:nrow(tmp),]

##############
## [NEED TO BE MODIFIED]
## vvvvvv TEMPORARY DATAFRAME THAT DUPLICATES EDDYDATA TO MAKE 3-MONTH-LONG DATAFRAME. REMOVE AFTER AUGUST!!!!
##############
#TEMPEddyDataWithPosix.F <- rbind(EddyDataWithPosix.F,EddyDataWithPosix.F)
#rownames(TEMPEddyDataWithPosix.F) <- seq(1:dim(TEMPEddyDataWithPosix.F)[1])
#TEMPEddyDataWithPosix.F <- rbind(EddyDataWithPosix.F,EddyDataWithPosix.F,EddyDataWithPosix.F)
#last_idx <- length(EddyData.F$Year)

#for (i in 2:length(TEMPEddyDataWithPosix.F$DoY)) {
#  prev_hour <- TEMPEddyDataWithPosix.F$Hour[i-1]
#  prev_day <- TEMPEddyDataWithPosix.F$DoY[i-1]
#  prev_year <- TEMPEddyDataWithPosix.F$Year[i-1]
#  TEMPEddyDataWithPosix.F$DoY[i] <- prev_day
#  TEMPEddyDataWithPosix.F$Year[i] <- prev_year
#  # Hour
#  if (prev_hour != 23.5) {
#    TEMPEddyDataWithPosix.F$Hour[i] <- prev_hour+0.5
#  } else {
#    TEMPEddyDataWithPosix.F$Hour[i] <- 0.0
#  }
#  # Day
#  if (prev_day != 365 & prev_hour == 23.5) {
#    TEMPEddyDataWithPosix.F$DoY[i] <- prev_day + 1
#  } else if (prev_day == 365 & prev_hour == 23.5) {
#    TEMPEddyDataWithPosix.F$DoY[i] <- 1
#    TEMPEddyDataWithPosix.F$Year[i] <- prev_year+1
#  }
#}
#EddyDataWithPosix.F <- fConvertTimeToPosix(TEMPEddyDataWithPosix.F, 'YDH',Year.s = 'Year',Day.s = 'DoY',Hour.s = 'Hour')
##############
# ^^^^^^^ TEMPORARY DATAFRAME THAT DUPLICATES EDDYDATA TO MAKE 3-MONTH-LONG DATAFRAME. REMOVE AFTER AUGUST!!!!
##############





# Initalize R5 reference class sEddyProc for post-processing of eddy data with the variables needed for post-processing later -------------------------------------
EddyProc.C <- sEddyProc$new(siteID, EddyDataWithPosix.F,
														c("NEE","FCH4","LE","H","Ustar","Rg","Tair","RH","VPD",
														  "Ts_1_5cm","Ts_1_10cm","Ts_1_20cm","Ts_1_50cm",
														  "Ts_2_5cm","Ts_2_10cm","Ts_2_20cm","Ts_2_50cm",
														  'Cond','DO','WL','TW','pH','ORP'))
														  
														  #"Ts_2_5cm","Ts_2_10cm","Ts_2_20cm","Ts_2_50cm",
														  #'Cond','DO','WL','TW','pH','ORP'))

uStarTh <- EddyProc.C$sEstUstarThresholdDistribution(nSample = 100L, probs = c(0.05, 0.5, 0.95)) #added 14/12/2020 M.Nyberg
uStarTh %>%
  filter( aggregationMode == "year") %>%
  select( uStar, "5%", "50%", "95%") #added 14/12/2020 M.Nyberg

uStarThAnnual <-usGetAnnualSeasonUStarMap(uStarTh)[-2] #added 14/12/2020 M.Nyberg
uStarSuffixes <- colnames(uStarThAnnual)[-1] #added 14/12/2020 M.Nyberg
print(uStarThAnnual) #added 14/12/2020 M.Nyberg

EddyProc.C$sGetUstarScenarios() #added 22/03/2021 M.Nyberg
EddyProc.C$sMDSGapFillUStarScens('NEE', FillAll = TRUE) #added M.Nyberg 26/03/2021
EddyProc.C$sMDSGapFillUStarScens('FCH4', FillAll = TRUE) #added M.Nyberg 26/03/2021


grep("NEE_.*_f$",names(EddyProc.C$sExportResults()) #added M.Nyberg 26/03/2021
, value = TRUE)
grep("NEE_.*_fsd$",names(EddyProc.C$sExportResults()) #added M.Nyberg 26/03/2021
, value = TRUE)


# Use MDS for gap-filling NEE, H, LE, and FCH4 (but also use RF for FCH4) ----------------------------------
EddyProc.C$sMDSGapFill('NEE', FillAll.b = TRUE)
EddyProc.C$sMDSGapFill('LE', FillAll.b = TRUE)
EddyProc.C$sMDSGapFill('H', FillAll.b = TRUE)
EddyProc.C$sMDSGapFill('FCH4', FillAll.b = TRUE)



# Gap-filling met data for partitioning  ----------------------------------
EddyProc.C$sSetLocationInfo(Lat_deg.n = 49.1245, Long_deg.n = -123.1922, TimeZone_h.n = -8)
EddyProc.C$sMDSGapFill('Tair', FillAll.b = FALSE)
EddyProc.C$sMDSGapFill('VPD', FillAll.b = FALSE)
EddyProc.C$sMDSGapFill('Rg', FillAll.b = FALSE)
EddyProc.C$sMDSGapFill('Ts_1_5cm', FillAll.b = FALSE)
EddyProc.C$sMDSGapFill('Ts_1_10cm', FillAll.b = FALSE)
EddyProc.C$sMDSGapFill('Ts_1_20cm', FillAll.b = FALSE)
EddyProc.C$sMDSGapFill('Ts_1_50cm', FillAll.b = FALSE)
EddyProc.C$sMDSGapFill('Ts_2_5cm', FillAll.b = FALSE)
EddyProc.C$sMDSGapFill('Ts_2_10cm', FillAll.b = FALSE)
EddyProc.C$sMDSGapFill('Ts_2_20cm', FillAll.b = FALSE)
EddyProc.C$sMDSGapFill('Ts_2_50cm', FillAll.b = FALSE)
EddyProc.C$sMDSGapFill('Cond', FillAll.b = FALSE)
EddyProc.C$sMDSGapFill('DO', FillAll.b = FALSE)
EddyProc.C$sMDSGapFill('WL', FillAll.b = FALSE)
EddyProc.C$sMDSGapFill('TW', FillAll.b = FALSE)
EddyProc.C$sMDSGapFill('pH', FillAll.b = FALSE)
EddyProc.C$sMDSGapFill('ORP', FillAll.b = FALSE)

# Apply nighttime gap-filling algorithm -----------------------------------
EddyProc.C$sMRFluxPartition()

resP <- lapply(uStarSuffixes, function(suffix){ #added M.Nyberg 26/03/2021
EddyProc.C$sMRFluxPartition(Suffix.s = suffix)
})

grep("GPP.*_f$|Reco", names(EddyProc.C$sExportResults()), value = TRUE) #added M.Nyberg 26/03/2021

#EddyProc.C$sApplyUStarScen(EddyProc.C$sMRFluxPartition )

EddyProc.C$sPlotFingerprintY('GPP_U50_f', Year = 2022)


# Apply daytime gap-filling algorithm -----------------------------------
EddyProc.C$sGLFluxPartition()

dayP <- lapply(uStarSuffixes, function(suffix){ #added M.Nyberg 26/03/2021
EddyProc.C$sGLFluxPartition(Suffix.s = suffix)
})

grep("GPP.*_f$|Reco|", names(EddyProc.C$sExportResults()), value = TRUE) #added M.Nyberg 26/03/2021


# Output results -----------------------------------
FilledEddyData.F <- EddyProc.C$sExportResults()
names(FilledEddyData.F)
```

```{r Gap-fill and partition in REddyProc_2, echo=FALSE, include=FALSE}
# Plot results -----------------------------------

# daily sums (from REddyProc) - CHECK UNITS!
setwd(paste(Rdir,'/flux_data',sep=''))
EddyProc.C$sPlotDailySums(Var.s = 'LE_f', Format.s = "png", unit.s = "MJ/m2/day")
EddyProc.C$sPlotDailySums(Var.s = 'H_f', Format.s = "png", unit.s = "MJ/m2/day")
EddyProc.C$sPlotDailySums(Var.s = 'NEE_f', Format.s = "png", unit.s = "gC/m2/day")
EddyProc.C$sPlotDailySums(Var.s = 'FCH4_f', Format.s = "png", unit.s = "gC/m2/day")
EddyProc.C$sPlotDailySums(Var.s = 'GPP_f', Format.s = "png", unit.s = "gC/m2/day")
EddyProc.C$sPlotDailySums(Var.s = 'Reco', Format.s = "png", unit.s = "gC/m2/day")
EddyProc.C$sPlotDailySums(Var.s = 'GPP_DT', Format.s = "png", unit.s = "gC/m2/day")
EddyProc.C$sPlotDailySums(Var.s = 'Reco_DT', Format.s = "png", unit.s = "gC/m2/day")
# Could create other plots to check the results
#....

```

```{r Save full REddyPro output, echo=FALSE, include=FALSE}
write_csv(FilledEddyData.F,paste(Rdir,'/flux_data/REddyProc_gapfilled_partition_fulloutput',siteID,'.csv',sep=''))
```

```{r Save only essential variables, echo=FALSE, include=FALSE}
essential_variables <- grep("NEE_f$|NEE_fsd$|GPP_f$|GPP_DT.*$|Reco|Reco_DT.*$|LE_f$|LE_fsd$|H_f$|H_fsd$|FCH4_f$|FCH4_uStar$|FCH4_uStar_f|FCH4_fsd$|NEE_uStar_f|NEE_U05_f|NEE_U50_f|NEE_U95_f|NEE_uStar_fsd|NEE_U05_fsd|NEE_U50_fsd|NEE_U95_fsd",
														names(EddyProc.C$sExportResults()), value = TRUE)
essential_variables

# # Remove WTH
# essential_variables <- str_remove(essential_variables, "WTH_.*")

essential <- FilledEddyData.F[,which(names(FilledEddyData.F) %in% essential_variables)]
```

```{r gap-fill FCH4 using random forest from Kim et al. 2019, echo=FALSE, warning=FALSE}

# variable we need for FCH4 gap-filling
Input <- read.table(paste(Rdir,'/flux_data/REddyProc_input/for_gap_filling_partitioning',siteID,'.txt',sep=''), header = T)

# Delete first row that contains units
Input <- Input[-1, ]
Input <- data.frame(lapply(Input, function(x) as.numeric(as.character(x))))

Input$HH <- floor(Input$Hour)
Input$MM <- (Input$Hour-Input$HH)*60

# Create time stamp
Input$TIMESTAMP_END <- make_datetime(Input$Year, 1, Input$DoY, Input$HH, Input$MM)

# Define predictors
#predictors <- c("FCH4", "Ustar","NEE","LE","H","Rg","Tair","RH","VPD","DoY",
#                "Ts_1_5cm","Ts_1_10cm","Ts_1_20cm","Ts_1_50cm",
#                "Ts_2_5cm","Ts_2_10cm","Ts_2_20cm","Ts_2_50cm") 

# Added water measurements
predictors <- c("FCH4", "Ustar","NEE","LE","H","Rg","Tair","RH","VPD","DoY",
                "Ts_1_5cm","Ts_1_10cm","Ts_1_20cm","Ts_1_50cm",
                "Cond","WL","TW",
                "Ts_2_5cm","Ts_2_10cm","Ts_2_20cm","Ts_2_50cm")
#                ,"Cond","DO","WL","TW","pH","ORP") 

ML.df <- Input %>% select(predictors)

# Replace all -9999 with NA
ML.df[ML.df == -9999] <- NA

# Add sine and cosine functions
ML.df$s <- sin((ML.df$DoY-1)/365*2*pi)
ML.df$c <- cos((ML.df$DoY-1)/365*2*pi)

# period when FCH4 is not missing
wm_only <- ML.df[!is.na(ML.df$FCH4), ]

############### Random forest run 20x ###############

#Add parallel processing for the fast processing if you want to
library(parallel)
library(doParallel)
library(caret)
library(data.table)

combined_result <- list()

for(i in 1:20){
  
  start_time <- Sys.time()
  # setting seed
  set.seed(i)
  train_rows <- sample(1:nrow(wm_only), 0.75*nrow(wm_only))
  # select the training set
  train_set <- wm_only %>% slice(train_rows)
  # select the validation set
  test_set <- anti_join(wm_only, train_set)
  #### option 1. random forest model with mtry tuning
  #cluster <- makeCluster(6)
  cluster <- parallel::makeCluster(10, setup_timeout = 0.5)
  registerDoParallel(cluster)
  RF_FCH4 <- train(FCH4 ~ ., data = train_set[,predictors],
   								 method = "rf",
   								 preProcess = c("medianImpute"),                #impute missing met data with median
   								 trControl=trainControl(method = "cv",   #three-fold cross-validation for model parameters 3 times
   								 											number = 3),
   								 na.action = na.pass,
   								 # allowParallel=FALSE, # This requires parallel packages. Otherwise you can choose FALSE.
   								 ntree=400, # can generate more trees
   								 importance = TRUE)
  
  RF_FCH4$bestTune
  RF_FCH4$results
  
  ############### Results
  # Variable importance
  print(plot(varImp(RF_FCH4, scale = FALSE), 
             main=paste("[Test - ",i,"] variable importance",sep=""))) 
  
  
  
  # Generate FCH4_rf predictions for testset
  test_set$FCH4_rf <- predict(RF_FCH4, test_set, na.action = na.pass)
  regrRF <- lm(test_set$FCH4_rf ~ test_set$FCH4); 
  print("RF Summary of test data:")
  print(summary(regrRF))
  
  my.formula <- y ~ x
  print(ggplot(test_set, aes(x=FCH4, y=FCH4_rf)) + 
    geom_abline(slope = 1, intercept = 0, aes(colour="identity line"))+
    geom_point() + geom_smooth(method = "lm", formula = my.formula, aes(colour="linear model")) + 
    scale_color_manual(values = c("identity line" = "black", "linear model" = "blue"))+
    stat_poly_eq(formula = my.formula, aes(label = paste(..eq.label.., ..rr.label.., sep = "~~~")),parse = TRUE) +
    ggtitle(paste("[Test - ",i,"] Test result",sep=""))) 
    

    
  # whole dataset
  result <- data.frame(FCH4 = ML.df$FCH4) # you can add datetime column here if you want to.
  result$FCH4_RF_model <- predict(RF_FCH4, ML.df, na.action = na.pass) # FCH4 RF model
  result$FCH4_RF_filled <- ifelse(is.na(result$FCH4),result$FCH4_RF_model,result$FCH4) 
  # gap-filled column (true value when it is, gap-filled value when missing)
  result$FCH4_RF_residual <- ifelse(is.na(result$FCH4),NA,result$FCH4_RF_model - result$FCH4) 
  # residual (model - obs). can be used for random uncertainty analysis
  
  # time series
  result$DateTime <- Input$TIMESTAMP_END
  
  #result %>% ggplot(aes(DateTime,FCH4)) + geom_point() + 
  #  theme_bw() + ylab(expression(paste("FCH4 (umol ", m^-2,s^-1,")"))) %>% 
  #  print()

  #  result %>% ggplot(aes(DateTime,FCH4_RF_filled)) + 
  #  geom_point(color="red",alpha=0.5) +
  #  geom_point(aes(DateTime,FCH4),color="black")+
  #  theme_bw() + ylab(expression(paste("FCH4 (umol ", m^-2,s^-1,")"))) %>% 
  #  print()
    
  print(result %>% ggplot(aes(DateTime,FCH4)) + geom_point() + 
    theme_bw() + ylab(expression(paste("FCH4 (umol ", m^-2,s^-1,")",sep=""))) +
    ggtitle(paste("FCH4 original time series",sep="")))
  
  print(result %>% ggplot(aes(DateTime,FCH4_RF_filled)) + 
    geom_point(aes(color="FCH4_RF_Filled"),alpha=0.5) +
    geom_point(aes(DateTime,FCH4,color="FCH4"))+
    scale_color_manual(values = c("FCH4_RF_Filled" = "red", "FCH4" = "black"))+
    theme_bw() + ylab(expression(paste("FCH4 (umol ", m^-2,s^-1,")",sep=""))) +
    ggtitle(paste("FCH4 gap-filled time series",sep="")))

  
  # whole data comparison
  print(ggplot(result, aes(x = FCH4, y =FCH4_RF_model)) + 
          geom_abline(slope = 1, intercept = 0, aes(colour="identity line"))+
          geom_point() + geom_smooth(method = "lm", formula = my.formula, aes(colour="linear model")) + 
          stat_poly_eq(formula = my.formula, aes(label = paste(..eq.label.., ..rr.label.., sep = "~~~")),parse = TRUE)+
          ggtitle(paste("[Test - ",i,"] whole dataset",sep=""))+
    scale_color_manual(values = c("identity line" = "black", "linear model" = "blue")))
  
  regrRF_whole <- lm(result$FCH4_RF_model ~ result$FCH4);
  print("RF Summary of whole data:")
  print(summary(regrRF_whole))
  
  
  result$iteration <- i 
  combined_result[[i]] <- result
  end_time <- Sys.time()
  end_time - start_time
}

rf_result_df <- data.table::rbindlist(combined_result)
write_csv(rf_result_df,paste(Rdir,'/flux_data/',siteID,'_rf_result.csv',sep=''))

```



```{r Load L2 data, echo=FALSE, include=FALSE}
L2 <- read.csv(paste(Rdir,'/flux_data/',siteID,'_L2.csv',sep = ''),header=TRUE,dec=".")
met <- read.csv(paste(Rdir,'/met_data/Gapfilled_biomet.csv',sep = ''))

#L2 <- L2[iStart:nrow(tmp),]
#met <- met[iStart:nrow(tmp),]

# Calculate salinity from conductivity
met$Salinity <- ((met$COND_1_1_1/1000)^1.0878)*0.4665

# Convert Air Pressure from Pa -> kPa
met$PA_1_1_1 <- met$PA_1_1_1/1000

names(met)[names(met) == "DOY"] <- "doy_int"
# Exclude duplicate variables 
met <- select(met,-c(date,time,Year_local,jday_local,month_local,hour_local,time_local,DOY_local,min_local))

# Reorder columns
  # since the order changes from to time, use variable names to reorder.
  #met <- met[,c(1,2,58,59,65,3,4,66,5:57)]
met <- select(met,c(ALB_1_1_1,COND_1_1_1, Salinity, DO_1_1_1:PA_1_1_1, pH_1_1_1,
                     PPFD_2_1_1,PPFDR_1_1_1:PPFD_2_1_1,
                     PPFDD_1_1_1,PRI_1_1_1:nm570out_1_1_1,
                     year,doy_int,DATE,hour,NR,G,AE,es,VPD,obs1))





# First, RF CH4 gap-filled data (figure out uncertainty for RF gap-filling)
#L2$FCH4_gf_RF <- result$FCH4_RF_filled[iStart:nrow(tmp)]
L2$FCH4_gf_RF <- result$FCH4_RF_filled

# REddyProc data 
head(L2)
head(essential)

################################################
# TEMP for Data no longer than three month
essential <- essential[1:dim(L2)[1],]
################################################


# Check to make sure files are of the same length and if TRUE, append to L2
if (nrow(L2) == nrow(essential)){
  # If TRUE, Combine L2 and essential-flux
  L3 <- cbind(L2,essential)
}


# Merge flux and met data
L3 <- merge(L3, met, by = "DATE") 

write_csv(L3,paste(Rdir,'/Flux_data/',siteID,'_L3.csv',sep=''))

```

```{r Uncertainty estimation, echo=FALSE}
# Uncertainty analysis ----------------------------- #added TYL 2022DEC20
uncInput <- cbind(EddyDataWithPosix.F$DateTime,
                  EddyDataWithPosix.F$Year,
                  #EddyDataWithPosix.F$DoY,
                  FilledEddyData.F)
colnames(uncInput)[names(uncInput)=='EddyDataWithPosix.F$DateTime'] <- "DateTime"
colnames(uncInput)[names(uncInput)=='EddyDataWithPosix.F$Year'] <- "Year"
#colnames(uncInput)[names(uncInput)=='EddyDataWithPosix.F$DoY'] <- "DoY"

#YrMx <- ceiling(nrow(uncInput)/(48*365))
#YrLst <- unique(uncInput$Year)

bt <- as_datetime("2022-06-01 15:00:00 UTC") # begin time
et <- as_datetime("2023-04-03 23:59:59 UTC") # end time

bts <- format(bt, "%Y%m%d")
ets <- format(et, "%Y%m%d")

# (1) NEE
NEEunc <- uncInput %>% 
  mutate(
    NEE_orig_sd = ifelse(
      is.finite(NEE_uStar_orig), NEE_uStar_fsd, NA), # NEE_orig_sd includes NEE_uStar_fsd only for measured values
    NEE_uStar_fgood = ifelse(
      NEE_uStar_fqc <= 1, is.finite(NEE_uStar_f), NA), # Only include filled values for the most reliable gap-filled observations. Note that is.finite() shouldn't be used here.
    resid = ifelse(NEE_uStar_fqc == 0, NEE_uStar_orig - NEE_uStar_fall, NA)) # quantify the error terms, i.e. model-data residuals (only using observations and exclude also

results <- tibble() # For storing results of annual sums and uncertainties

NEEuncYr <- NEEunc[NEEunc$DateTime>=bt & NEEunc$DateTime<=et,]
  #if (i==YrMx) {ep<-nrow(uncInput)}else{ep <- i*48*365}
  #NEEuncYr <- NEEunc[bp:ep,]
  
  
  #NEEuncYr <- NEEunc[NEEunc$Year==i,]
  
  autoCorr <- lognorm::computeEffectiveAutoCorr(NEEuncYr$resid)
  nEff <- lognorm::computeEffectiveNumObs(NEEuncYr$resid, na.rm = TRUE)
  c(nEff = nEff, nObs = sum(is.finite(NEEuncYr$resid))) 
  
  #seg_1 <- tibble(uncInput$DateTime[bp],uncInput$DateTime[ep],nrow(NEEuncYr),nEff,sum(is.finite(NEEuncYr$resid)))
  seg_1 <- tibble(head(NEEuncYr$DateTime,1),tail(NEEuncYr$DateTime,1),nrow(NEEuncYr),nEff,sum(is.finite(NEEuncYr$resid)))
  
  resRand <- NEEuncYr %>% summarise(
    nRec = sum(is.finite(NEE_orig_sd))
    , NEEagg = mean(NEE_uStar_f, na.rm = TRUE)
    , varMean = sum(NEE_orig_sd^2, na.rm = TRUE) / nRec / (!!nEff - 1)
    , sdMean = sqrt(varMean) 
    , sdMeanApprox = mean(NEE_orig_sd, na.rm = TRUE) / sqrt(!!nEff - 1)
    ) %>% select(NEEagg, sdMean, sdMeanApprox)
  
  # u* threshold uncertainty
  ind <- which(grepl("NEE_U*", names(NEEuncYr)) & grepl("_f$", names(NEEuncYr)))
  column_name <- names(NEEuncYr)[ind] 
  
  #calculate column means of specific columns
  NEEagg <- colMeans(NEEuncYr[ ,column_name], na.rm=T)
  NEEagg
  
  #compute uncertainty across aggregated values
  sdNEEagg_ustar <- sd(NEEagg)
  sdNEEagg_ustar
  
  # Combined aggregated uncertainty
  #Assuming that the uncertainty due to unknown u*threshold is independent from the random uncertainty, the variances add.
  NEE_sdAnnual <- data.frame(
    sd_NEE_Rand = resRand$sdMean,
    sd_NEE_Ustar = sdNEEagg_ustar,
    sd_NEE_Comb = sqrt(resRand$sdMean^2 + sdNEEagg_ustar^2) 
    )
  NEE_sdAnnual
  
  data.mean_NEE_uStar_f <- data.frame(mean(NEEuncYr$NEE_uStar_f, na.rm = TRUE))
  colnames(data.mean_NEE_uStar_f) <- 'mean_NEE_uStar_f'
  NEE_sdAnnual <- cbind(data.mean_NEE_uStar_f,NEE_sdAnnual)
  
  # Convert to annual sums
  conv_gCO2 <- 1/(10^6)*44.01*60*60*24*length(NEEuncYr$NEE_uStar_f)/48 # Converts umol to mol, mol to gCO2, x seconds in a year
  conv_gC <- 1/(10^6)*12.011*60*60*24*length(NEEuncYr$NEE_uStar_f)/48  # Converts umol to mol, mol to gC,   x seconds in a year
  
  # g CO2
  mean_sdAnnual_gCO2 <- NEE_sdAnnual*conv_gCO2
  mean_sdAnnual_gCO2
  # g C
  mean_sdAnnual_gC <- NEE_sdAnnual*conv_gC
  mean_sdAnnual_gC
  
  segs <- cbind(seg_1,as.tibble(mean_sdAnnual_gCO2),as.tibble(mean_sdAnnual_gC))
    results <- rbind(results,segs)
    


colnames(results) <- c('StartDate','EndDate','AvailNum','EffNum','ObsNum',
                       'AnnNEE_gCO2','UncNEE_Rand_gCO2','UncNEE_uStar_gCO2','UncNEE_Comb_gCO2',
                       'AnnNEE_gC','UncNEE_Rand_gC','UncNEE_uStar_gC','UncNEE_Comb_gC')
print(results, quote = FALSE, row.names = TRUE)


outname <-paste(Rdir,'/Flux_data/',siteID,'_NEE_uncertainty_',bts,'-',ets,'.csv',sep="")
write.csv(as.matrix(results),outname, row.names = FALSE)

# (2) FCH4


FCH4unc <- uncInput  %>% 
  mutate(
    FCH4_orig_sd = ifelse(
      is.finite(FCH4_uStar_orig), FCH4_uStar_fsd, NA), # NEE_orig_sd includes NEE_uStar_fsd only for measured values
    FCH4_uStar_fgood = ifelse(
      FCH4_uStar_fqc <= 1, is.finite(FCH4_uStar_f), NA), # Only include filled values for the most reliable gap-filled observations. Note that is.finite() shouldn't be used here.
    resid = ifelse(FCH4_uStar_fqc == 0, FCH4_uStar_orig - FCH4_uStar_fall, NA)) # quantify the error terms, i.e. model-data residuals (only using observations and exclude also

results <- tibble() # For storing results of annual sums and uncertainties

FCH4uncYr <- FCH4unc[FCH4unc$DateTime>=bt & FCH4unc$DateTime<=et,]
  #bp <- (i-1)*48*365+1
  #if (i==YrMx) {ep<-nrow(uncInput)}else{ep <- i*48*365}
  
  #FCH4uncYr <- FCH4unc[bp:ep,]
  #FCH4uncYr <- FCH4unc[FCH4unc$Year==i,]
  autoCorr  <- lognorm::computeEffectiveAutoCorr(FCH4uncYr$resid)
  nEff      <- lognorm::computeEffectiveNumObs(FCH4uncYr$resid, na.rm = TRUE)
  c(nEff = nEff, nObs = sum(is.finite(FCH4uncYr$resid)))
  #seg_1 <- tibble(uncInput$DateTime[bp],uncInput$DateTime[ep],nrow(FCH4uncYr),nEff,sum(is.finite(FCH4uncYr$resid)))
  seg_1 <- tibble(head(FCH4uncYr$DateTime,1),tail(FCH4uncYr$DateTime,1),nrow(FCH4uncYr),nEff,sum(is.finite(FCH4uncYr$resid)))
  resRand <- FCH4uncYr %>% summarise(
    nRec = sum(is.finite(FCH4_orig_sd))
    , FCH4agg = mean(FCH4_uStar_f, na.rm = TRUE)
    , varMean = sum(FCH4_orig_sd^2, na.rm = TRUE) / nRec / (!!nEff - 1)
    , sdMean = sqrt(varMean) 
    , sdMeanApprox = mean(FCH4_orig_sd, na.rm = TRUE) / sqrt(!!nEff - 1)
    ) %>% select(FCH4agg, sdMean, sdMeanApprox)
  
  # u* threshold uncertainty
  ind <- which(grepl("FCH4_U*", names(FCH4uncYr)) & grepl("_f$", names(FCH4uncYr)))
  column_name <- names(FCH4uncYr)[ind] 
  
  #calculate column means of specific columns
  FCH4agg <- colMeans(FCH4uncYr[ ,column_name], na.rm=T)
  FCH4agg
  
  #compute uncertainty across aggregated values
  sdFCH4agg_ustar <- sd(FCH4agg)
  sdFCH4agg_ustar
  
  # Combined aggregated uncertainty
  #Assuming that the uncertainty due to unknown u*threshold is independent from the random uncertainty, the variances add.
  FCH4_sdAnnual <- data.frame(
    sd_FCH4_Rand = resRand$sdMean,
    sd_FCH4_Ustar = sdFCH4agg_ustar,
    sd_FCH4_Comb = sqrt(resRand$sdMean^2 + sdFCH4agg_ustar^2) 
    )
  
  FCH4_sdAnnual
  data.mean_FCH4_uStar_f <- data.frame(mean(FCH4uncYr$FCH4_uStar_f, na.rm = TRUE))
  colnames(data.mean_FCH4_uStar_f) <- 'mean_FCH4_uStar_f'
  FCH4_sdAnnual <- cbind(data.mean_FCH4_uStar_f,FCH4_sdAnnual)
  
  # Create output data frame
  mean_sdAnnual <- FCH4_sdAnnual
  
  # Convert to annual sums
  conv_gCH4 <- 1/(10^6)*16.04*60*60*24*length(FCH4uncYr$FCH4_uStar_f)/48 # Converts umol to mol, mol to mgCH4, x seconds in a year
  conv_gC <- 1/(10^6)*12.011*60*60*24*length(FCH4uncYr$FCH4_uStar_f)/48  # Converts umol to mol, mol to mgC,   x seconds in a year
  
  # g CH4
  mean_sdAnnual_gCH4 <- mean_sdAnnual*conv_gCH4
  mean_sdAnnual_gCH4
  
  # g C
  mean_sdAnnual_gC <- mean_sdAnnual*conv_gC
  mean_sdAnnual_gC
  
  segs <- cbind(seg_1,as.tibble(mean_sdAnnual_gCH4),as.tibble(mean_sdAnnual_gC))
  results <- rbind(results,segs)


colnames(results) <- c('StartDate','EndDate','AvailNum','EffNum','ObsNum',
                       'AnnFCH4_gCH4','UncFCH4_Rand_gCH4','UncFCH4_uStar_gCH4','UncFCH4_Comb_gCH4',
                       'AnnFCH4_gC','UncFCH4_Rand_gC','UncFCH4_uStar_gC','UncFCH4_Comb_gC')
print(results, quote = FALSE, row.names = TRUE)


outname <-paste(Rdir,'/Flux_data/',siteID,'_FCH4_uncertainty_',bts,'-',ets,'.csv',sep="")
write.csv(as.matrix(results),outname, row.names = FALSE)

```



